# -*- coding: utf-8 -*-
"""PFC2 Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vJNExo1fmAppfJW-v4uEjBluSxpEjXzz
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""**ANÁLISIS DEL DATASET**"""



# Config graficos
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# Dataset
ruta_dataset = 'weatherAUS.csv'
data = pd.read_csv(ruta_dataset)

# Primeras filas
data.head()

"""- Revisando valores nulos"""

# Porcentaje de valores nulos
nulos = data.isnull().mean() * 100

# Mostrar ordenado de mayor a menor
nulos_ordenados = nulos.sort_values(ascending=False)
print(nulos_ordenados)

# Grafica
plt.figure(figsize=(14, 6))
sns.barplot(x=nulos_ordenados.index, y=nulos_ordenados.values,hue=nulos_ordenados.index, palette="viridis",legend=False)
plt.xticks(rotation=90)
plt.ylabel('Porcentaje de valores nulos (%)')
plt.title('Porcentaje de valores nulos por característica')
plt.show()

"""Descartando columnas con valores nulos en su mayoria"""

# Columnas a eliminar
columnas_a_eliminar = ['Sunshine', 'Evaporation', 'Cloud9am', 'Cloud3pm']

# Eliminar
data.drop(columns=columnas_a_eliminar, inplace=True)

print("Columnas eliminadas:", columnas_a_eliminar)
print("Shape actualizado:", data.shape)

"""- Matriz de correlación entre dos valores"""

# Solo variables numéricas
columnas_a_excluir = ['Date', 'Location', 'RainToday', 'RainTomorrow']
columnas_numéricas = data.select_dtypes(include=[np.number]).columns.difference(columnas_a_excluir)

# Matriz de correlación
correlacion = data[columnas_numéricas].corr()

# Matriz de correlación como heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlacion, annot=True, cmap="coolwarm", center=0)
plt.title('Matriz de Correlación entre variables numéricas')
plt.show()

"""- Verificar outliers"""

# Variables continuas relevantes para ver outliers
variables_continuas = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed',
                       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',
                       'Humidity3pm', 'Pressure9am', 'Pressure3pm',
                       'Temp9am', 'Temp3pm']

# Crear boxplots para detectar outliers
plt.figure(figsize=(18, 10))

for i, variable in enumerate(variables_continuas, 1):
    plt.subplot(4, 3, i)
    sns.boxplot(y=data[variable], color="skyblue")
    plt.title(variable)

plt.tight_layout()
plt.show()

"""- Eliminación Outliers"""

# Variables numéricas con outliers
variables_continuas = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed',
                       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',
                       'Humidity3pm', 'Pressure9am', 'Pressure3pm',
                       'Temp9am', 'Temp3pm']

# Copia del dataset
data_wo_outliers = data.copy()

# Eliminación de outliers usando IQR para cada variable
for var in variables_continuas:
    Q1 = data_wo_outliers[var].quantile(0.25)
    Q3 = data_wo_outliers[var].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    # Filtrar filas que estén dentro del rango aceptado
    data_wo_outliers = data_wo_outliers[(data_wo_outliers[var] >= lower_bound) & (data_wo_outliers[var] <= upper_bound)]

print(f"Filas restantes después de eliminar outliers: {data_wo_outliers.shape[0]}")

# Convertir la fecha a datetime
data_wo_outliers['Date'] = pd.to_datetime(data_wo_outliers['Date'])

# Extraer mes
data_wo_outliers['Month'] = data_wo_outliers['Date'].dt.month

# Crear columna 'Season'
def asignar_estacion(mes):
    if mes in [12, 1, 2]:
        return 'Summer'
    elif mes in [3, 4, 5]:
        return 'Fall'
    elif mes in [6, 7, 8]:
        return 'Winter'
    else:
        return 'Spring'

data_wo_outliers['Season'] = data_wo_outliers['Month'].apply(asignar_estacion)

# Imputación por promedio agrupado por Location y Season
columnas_numericas = data_wo_outliers.select_dtypes(include=[np.number]).columns

for col in columnas_numericas:
    if data_wo_outliers[col].isnull().sum() > 0 and col not in ['Rainfall']:  # Rainfall se tratará distinto
        data_wo_outliers[col] = data_wo_outliers.groupby(['Location', 'Season'])[col]\
                                .transform(lambda x: x.fillna(x.mean()))

# Imputación por moda
columnas_categoricas = data_wo_outliers.select_dtypes(include='object').columns.difference(['Date'])

for col in columnas_categoricas:
    data_wo_outliers[col] = data_wo_outliers.groupby(['Location', 'Season'])[col]\
                            .transform(lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else np.nan))

# Manejo especial de datos de lluvia
data_wo_outliers['Rainfall'] = data_wo_outliers['Rainfall'].fillna(0)
data_wo_outliers = data_wo_outliers.dropna(subset=['RainToday', 'RainTomorrow'])

"""- 'Yes' and 'No' a 1 y 0"""

data_wo_outliers['RainToday'] = data_wo_outliers['RainToday'].map({'Yes': 1, 'No': 0})
data_wo_outliers['RainTomorrow'] = data_wo_outliers['RainTomorrow'].map({'Yes': 1, 'No': 0})

"""- Viento a valores numericos"""

direccion_a_grados = {
    'N': 0, 'NNE': 22.5, 'NE': 45.0, 'ENE': 67.5,
    'E': 90.0, 'ESE': 112.5, 'SE': 135.0, 'SSE': 157.5,
    'S': 180.0, 'SSW': 202.5, 'SW': 225.0, 'WSW': 247.5,
    'W': 270.0, 'WNW': 292.5, 'NW': 315.0, 'NNW': 337.5
}

columnas_direccion = ['WindGustDir', 'WindDir9am', 'WindDir3pm']
for col in columnas_direccion:
    data_wo_outliers[col] = data_wo_outliers[col].map(direccion_a_grados)

print("Data:\n",data_wo_outliers.head())

"""Balanceo de clase"""

from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.model_selection import train_test_split

# Variables independientes y objetivo
X = data_wo_outliers.drop(columns=['RainTomorrow','Date'])
y = data_wo_outliers['RainTomorrow']

# Separar en train/test (antes de aplicar SMOTE)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# distribución original de clases
print("Distribución original en y_train:", Counter(y_train))

# Aplicar SMOTE SOLO a X_train
smote = SMOTE(random_state=42)
# Eliminar columnas categóricas no codificadas antes de SMOTE
# o cualquier columna no numerica
X_train = X_train.drop(columns=['Location'])
X_test = X_test.drop(columns=['Location'])
X_train = X_train.drop(columns=['Season'])
X_test = X_test.drop(columns=['Season'])


X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

print("Distribución después de SMOTE:", Counter(y_train_bal))

import seaborn as sns
import matplotlib.pyplot as plt

# Antes de SMOTE
sns.countplot(x=y_train)
plt.title("Distribución antes de SMOTE")
plt.show()

# Después de SMOTE
sns.countplot(x=y_train_bal)
plt.title("Distribución después de SMOTE")
plt.show()

from sklearn.preprocessing import StandardScaler

# Eliminar columnas no numéricas o innecesarias para regresión
columnas_a_remover = ['Date', 'Location', 'RainToday', 'RainTomorrow', 'Season', 'Month']
df_regresion = data_wo_outliers.drop(columns=columnas_a_remover)

# Separar variables independientes y objetivo
X_reg = df_regresion.drop(columns=['Rainfall'])
y_reg = df_regresion['Rainfall']

# Dividir en train/test
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

# Normalizar usando StandardScaler
scaler = StandardScaler()
X_train_reg_scaled = scaler.fit_transform(X_train_reg)
X_test_reg_scaled = scaler.transform(X_test_reg)

# Tamaño general del dataset después del EDA
print("Tamaño final del dataset:", data_wo_outliers.shape)

# Para clasificación
print("Tamaño de entrenamiento con SMOTE:", X_train_bal.shape)
print("Tamaño de prueba:", X_test.shape)

# Para regresión
print("Tamaño entrenamiento (regresión):", X_train_reg.shape)
print("Tamaño prueba (regresión):", X_test_reg.shape)

"""ENTRANAMIENTO Y PREDICCIÓN"""

# ===========================================
# CLASIFICACIÓN RANDOM FOREST
# ===========================================
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# 1. Preprocesamiento de datos
X_class = data_wo_outliers.drop(columns=['Date', 'RainTomorrow', 'Rainfall'])  # Rainfall no se usa aquí
y_class = data_wo_outliers['RainTomorrow']

# One-hot encoding si hay variables categóricas
X_class = pd.get_dummies(X_class)

# 2. Separar en train/test antes de SMOTE
X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_class, y_class, stratify=y_class, test_size=0.2, random_state=42)

# 3. Balancear clases con SMOTE
smote = SMOTE(random_state=42)
X_train_c_bal, y_train_c_bal = smote.fit_resample(X_train_c, y_train_c)

# 4. Normalizar
scaler_c = StandardScaler()
X_train_c_scaled = scaler_c.fit_transform(X_train_c_bal)
X_test_c_scaled = scaler_c.transform(X_test_c)

# 5. Modelo de clasificación
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train_c_scaled, y_train_c_bal)

# Evaluación
y_pred_c = clf.predict(X_test_c_scaled)
print("Accuracy:", accuracy_score(y_test_c, y_pred_c))
print("Confusion Matrix:\n", confusion_matrix(y_test_c, y_pred_c))
print("Classification Report:\n", classification_report(y_test_c, y_pred_c))

"""REGRESIÓN"""

# ===========================================
# REGRESIÓN RANDOM FOREST (Rainfall)
# ===========================================

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# 1. Preprocesamiento
X_reg = data_wo_outliers.drop(columns=['Date', 'RainTomorrow'])  # Se queda Rainfall
y_reg = data_wo_outliers['Rainfall']

# One-hot encoding si aplica
X_reg = pd.get_dummies(X_reg)

# 2. Separar en train/test
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

# 3. Normalización
scaler_r = StandardScaler()
X_train_r_scaled = scaler_r.fit_transform(X_train_r)
X_test_r_scaled = scaler_r.transform(X_test_r)

# 4. Modelo de regresión
regr = RandomForestRegressor(n_estimators=100, random_state=42)
regr.fit(X_train_r_scaled, y_train_r)

# 5. Evaluación
y_pred_r = regr.predict(X_test_r_scaled)
print("MSE:", mean_squared_error(y_test_r, y_pred_r))
print("R² Score:", r2_score(y_test_r, y_pred_r))

"""CLASIFICACION XGBOOST"""

# ===========================================
# XGBoost CLASIFICACIÓN + RandomizedSearchCV
# ===========================================
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import RandomizedSearchCV
import matplotlib.pyplot as plt
import numpy as np

# 1. espacio de hiperparámetros
params_xgb = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth':    [3, 5, 7, 10],
    'learning_rate':[0.01, 0.05, 0.1, 0.3],
    'subsample':    [0.6, 0.8, 1.0],
    'colsample_bytree':[0.6, 0.8, 1.0]
}

# 2. XGBClassifier base
xgb_clf = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    random_state=42
)

# 3. RandomizedSearchCV para encontrar la mejor combinación
random_search = RandomizedSearchCV(
    estimator=xgb_clf,
    param_distributions=params_xgb,
    n_iter=20, # número de combinaciones a probar
    scoring='f1', # métrica principal
    cv=3, # validación cruzada 3-fold
    verbose=2,
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train_c_scaled, y_train_c_bal)
best_model = random_search.best_estimator_
print("Mejores hiperparámetros:", random_search.best_params_)

# 4. Predicción en test
y_pred_xgb_opt = best_model.predict(X_test_c_scaled)

# 5. Visualizar importancia de variables
xgb.plot_importance(best_model, max_num_features=15)
plt.title("Importancia de variables - XGBoost")
plt.show()

# 6. Métricas finales
print("Accuracy:", accuracy_score(y_test_c, y_pred_xgb_opt))
print("Confusion Matrix:\n", confusion_matrix(y_test_c, y_pred_xgb_opt))
print("Classification Report:\n", classification_report(y_test_c, y_pred_xgb_opt))

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# 1. Obtener las probabilidades predichas de clase 1 (lluvia)
y_proba = best_model.predict_proba(X_test_c_scaled)[:, 1]

# 2. Probar diferentes umbrales
umbrales = np.arange(0.1, 0.91, 0.05)
mejor_f1 = 0
mejor_umbral = 0.5  # por defecto

print("\nEvaluación por umbral de decisión:\n")
for umbral in umbrales:
    y_pred_thr = (y_proba > umbral).astype(int)
    f1 = f1_score(y_test_c, y_pred_thr)
    rec = recall_score(y_test_c, y_pred_thr)
    pre = precision_score(y_test_c, y_pred_thr)

    print(f"Umbral {umbral:.2f} → F1: {f1:.3f}, Recall: {rec:.3f}, Precision: {pre:.3f}")

    if f1 > mejor_f1:
        mejor_f1 = f1
        mejor_umbral = umbral

print(f"\n🔍 Mejor umbral encontrado: {mejor_umbral} con F1-score: {mejor_f1:.3f}")

# Aplicar el mejor umbral encontrado
y_pred_final = (y_proba > mejor_umbral).astype(int)

print("\nReporte final con umbral ajustado:")
print(confusion_matrix(y_test_c, y_pred_final))
print(classification_report(y_test_c, y_pred_final))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

# Obtener probabilidades de clase 1
y_proba = best_model.predict_proba(X_test_c_scaled)[:, 1]

# Umbrales a probar
umbrales = np.arange(0.1, 0.91, 0.05)

# Almacenar métricas
accuracies = []
precisions = []
recalls = []
f1_scores = []

print("Evaluación para distintos umbrales:\n")
for umbral in umbrales:
    y_pred_thr = (y_proba > umbral).astype(int)

    acc = accuracy_score(y_test_c, y_pred_thr)
    pre = precision_score(y_test_c, y_pred_thr)
    rec = recall_score(y_test_c, y_pred_thr)
    f1 = f1_score(y_test_c, y_pred_thr)

    accuracies.append(acc)
    precisions.append(pre)
    recalls.append(rec)
    f1_scores.append(f1)

    print(f"Umbral: {umbral:.2f} → Accuracy: {acc:.3f}, Precision: {pre:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}")

# Gráfica comparativa
plt.figure(figsize=(12, 6))
plt.plot(umbrales, accuracies, label='Accuracy')
plt.plot(umbrales, precisions, label='Precision (clase 1)')
plt.plot(umbrales, recalls, label='Recall (clase 1)')
plt.plot(umbrales, f1_scores, label='F1-score (clase 1)')
plt.xlabel('Umbral de decisión')
plt.ylabel('Métrica')
plt.title('Métricas de clasificación vs Umbral de decisión')
plt.legend()
plt.grid(True)
plt.show()


umbral_optimo = 0.35  # por ejemplo
y_pred_final = (y_proba > umbral_optimo).astype(int)

print(f"\nReporte final con umbral = {umbral_optimo}")
print(confusion_matrix(y_test_c, y_pred_final))
print(classification_report(y_test_c, y_pred_final))

"""REGRESION XGBOOST"""

# ==============================
# XGBoost REGRESIÓN (Rainfall)
# ==============================
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# 1. Volvemos a crear los sets SIN la columna RainToday
# (puedes ejecutar esta sección independientemente)
X_reg_adj = X_reg.drop(columns=['RainToday'])
X_train_r_adj, X_test_r_adj, y_train_r, y_test_r = train_test_split(
    X_reg_adj, y_reg, test_size=0.2, random_state=42)

# 2. Re-escalar con el StandardScaler ya definido (scaler_r)
X_train_r_scaled = scaler_r.fit_transform(X_train_r_adj)
X_test_r_scaled  = scaler_r.transform(X_test_r_adj)

# 3. XGBRegressor
xgb_reg = xgb.XGBRegressor(
    objective='reg:squarederror',
    eval_metric='rmse',
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb_reg.fit(X_train_r_scaled, y_train_r)

# 4. Predicción y evaluación
y_pred_xgb_r = xgb_reg.predict(X_test_r_scaled)
print("MSE:", mean_squared_error(y_test_r, y_pred_xgb_r))
print("R² Score:", r2_score(y_test_r, y_pred_xgb_r))